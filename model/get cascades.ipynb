{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "73db8c5a-c965-4ff7-860f-cddf318b6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, PreOrderIter, RenderTree\n",
    "from anytree.importer import DictImporter\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from bson import ObjectId\n",
    "\n",
    "from deepdiff import DeepDiff\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class Cascade:\n",
    "    def create_cascade(self, file_name, ground_truth):\n",
    "        # read json\n",
    "        # read each cascade\n",
    "        # save each cascade with it's ground truth to the database\n",
    "        # data structure\n",
    "        '''\n",
    "        {\n",
    "        'node_sequence' : []\n",
    "        'ground_truth' : boolean\n",
    "        }\n",
    "        '''\n",
    "        print('create_cascade: ', file_name, ground_truth )\n",
    "        file = open(file_name)\n",
    "        data = json.load(file)\n",
    "        \n",
    "        importer = DictImporter()\n",
    "        root = importer.import_(data)\n",
    "#         print(RenderTree(root))\n",
    "        \n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.create_database('tweets')\n",
    "        records = db['cascades']\n",
    "        for leaf in PreOrderIter(root, filter_ = lambda node: node.is_leaf):\n",
    "            print([node.name for node in list(leaf.path)])\n",
    "            try:\n",
    "                records.insert_one({'node_sequence' : [node.name for node in list(leaf.path)], 'ground_truth' : ground_truth})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "    def cascade_analysis(self):\n",
    "        # number of false and true cascades\n",
    "        # max false and true cascades\n",
    "        # mean false and true cascades\n",
    "        \n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.create_database('tweets')\n",
    "        records = db['cascades']\n",
    "        \n",
    "        print('false count: ', records.count_documents({'ground_truth': False}))\n",
    "        lengths = [len(record['node_sequence']) for record in records.find({'ground_truth': False})]\n",
    "        print('false average number of nodes:', sum(lengths)/len(lengths))\n",
    "        print('false max number of nodes', max(lengths))\n",
    "        print(\"*\"*5)\n",
    "        print('True count: ', records.count_documents({'ground_truth': True}))\n",
    "        lengths = [len(record['node_sequence']) for record in records.find({'ground_truth': True})]\n",
    "        print('true average number of nodes:', sum(lengths)/len(lengths)) \n",
    "        print('true max number of nodes', max(lengths))\n",
    "    \n",
    "    def flattern(self, lis):\n",
    "        result = []\n",
    "        for item in lis:\n",
    "            if hasattr(item, '__iter__') and not isinstance(item, str):\n",
    "                result.extend(self.flattern(item))\n",
    "            else:\n",
    "                result.append(item)\n",
    "        return result\n",
    "        \n",
    "    def map_node_features(self, emotional_words):\n",
    "        emotional_vector = {\n",
    "             'fear': 0.0,\n",
    "             'anger': 0.0,\n",
    "             'anticipation': 0.0,\n",
    "             'trust': 0.0,\n",
    "             'surprise': 0.0,\n",
    "             'positive': 0.0,\n",
    "             'negative': 0.0,\n",
    "             'sadness': 0.0,\n",
    "             'disgust': 0.0,\n",
    "             'joy': 0.0\n",
    "        }\n",
    "        \n",
    "        # return default emotional vector if the emotional_words list is empty\n",
    "        if not bool(emotional_words):\n",
    "            return emotional_vector\n",
    "        \n",
    "        all_emotional_words = self.flattern([emotional_words[key] for key in emotional_words])\n",
    "        for emotion in all_emotional_words:\n",
    "            emotional_vector[emotion] += 1/len(all_emotional_words) \n",
    "        return emotional_vector  \n",
    "        \n",
    "    def create_node(self, node_id):\n",
    "        # read database\n",
    "        # return all emotionl words and entities\n",
    "        # populate_cascade\n",
    "        '''\n",
    "        node_features_sequence: [{\n",
    "            'user_id': str \n",
    "            'emotional_frequecy': \n",
    "                {\n",
    "                 'fear': 0.14285714285714285,\n",
    "                 'anger': 0.2857142857142857,\n",
    "                 'anticip': 0.0,\n",
    "                 'trust': 0.0,\n",
    "                 'surprise': 0.0,\n",
    "                 'positive': 0.0,\n",
    "                 'negative': 0.42857142857142855,\n",
    "                 'sadness': 0.14285714285714285,\n",
    "                 'disgust': 0.0,\n",
    "                 'joy': 0.0\n",
    "                 },\n",
    "            'emotional_mean': \n",
    "                {\n",
    "                 'fear': 0.14285714285714285,\n",
    "                 'anger': 0.2857142857142857,\n",
    "                 'anticip': 0.0,\n",
    "                 'trust': 0.0,\n",
    "                 'surprise': 0.0,\n",
    "                 'positive': 0.0,\n",
    "                 'negative': 0.42857142857142855,\n",
    "                 'sadness': 0.14285714285714285,\n",
    "                 'disgust': 0.0,\n",
    "                 'joy': 0.0\n",
    "                 },\n",
    "            'emotional_standard_deviation': \n",
    "                {\n",
    "                 'fear': 0.14285714285714285,\n",
    "                 'anger': 0.2857142857142857,\n",
    "                 'anticip': 0.0,\n",
    "                 'trust': 0.0,\n",
    "                 'surprise': 0.0,\n",
    "                 'positive': 0.0,\n",
    "                 'negative': 0.42857142857142855,\n",
    "                 'sadness': 0.14285714285714285,\n",
    "                 'disgust': 0.0,\n",
    "                 'joy': 0.0\n",
    "                 },\n",
    "             'emotional_q1': \n",
    "                {\n",
    "                 'fear': 0.14285714285714285,\n",
    "                 'anger': 0.2857142857142857,\n",
    "                 'anticip': 0.0,\n",
    "                 'trust': 0.0,\n",
    "                 'surprise': 0.0,\n",
    "                 'positive': 0.0,\n",
    "                 'negative': 0.42857142857142855,\n",
    "                 'sadness': 0.14285714285714285,\n",
    "                 'disgust': 0.0,\n",
    "                 'joy': 0.0\n",
    "                 },\n",
    "             'emotional_q2': \n",
    "                {\n",
    "                 'fear': 0.14285714285714285,\n",
    "                 'anger': 0.2857142857142857,\n",
    "                 'anticip': 0.0,\n",
    "                 'trust': 0.0,\n",
    "                 'surprise': 0.0,\n",
    "                 'positive': 0.0,\n",
    "                 'negative': 0.42857142857142855,\n",
    "                 'sadness': 0.14285714285714285,\n",
    "                 'disgust': 0.0,\n",
    "                 'joy\n",
    "                 ': 0.0\n",
    "                 },\n",
    "             'emotional_q3': \n",
    "                {\n",
    "                 'fear': 0.14285714285714285,\n",
    "                 'anger': 0.2857142857142857,\n",
    "                 'anticip': 0.0,\n",
    "                 'trust': 0.0,\n",
    "                 'surprise': 0.0,\n",
    "                 'positive': 0.0,\n",
    "                 'negative': 0.42857142857142855,\n",
    "                 'sadness': 0.14285714285714285,\n",
    "                 'disgust': 0.0,\n",
    "                 'joy': 0.0\n",
    "                 },\n",
    "            'followers_count': int\n",
    "            'friends_count': int\n",
    "            'listed_count': int\n",
    "            'statuses_count': int\n",
    "            'created_at': Time\n",
    "        }]\n",
    "        '''\n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.get_database('tweets')\n",
    "        records = db['timeline_extended']\n",
    "        \n",
    "        # each node is a user\n",
    "        node = records.find_one({'user_id': node_id})\n",
    "        emotion_timeline = []\n",
    "        for tweet in node['timeline']:\n",
    "            emotional_words = tweet['full_text_nlp']['emotional_words']\n",
    "            emotional_words = ast.literal_eval(emotional_words)\n",
    "            emotion_timeline.append(self.map_node_features(emotional_words))\n",
    "        \n",
    "        emotional_vector = {\n",
    "             'fear': 0.0,\n",
    "             'anger': 0.0,\n",
    "             'anticipation': 0.0,\n",
    "             'trust': 0.0,\n",
    "             'surprise': 0.0,\n",
    "             'positive': 0.0,\n",
    "             'negative': 0.0,\n",
    "             'sadness': 0.0,\n",
    "             'disgust': 0.0,\n",
    "             'joy': 0.0\n",
    "        }\n",
    "#         print(len(emotion_timeline))\n",
    "        dup_emotional_vector = emotional_vector\n",
    "        \n",
    "        # emotional frequnecy\n",
    "        for key in emotional_vector:\n",
    "            emotional_vector[key] = sum(item[key] for item in emotion_timeline)\n",
    "        emotional_frequency = emotional_vector\n",
    "#         print(emotional_frequency)\n",
    "        emotional_vector = dup_emotional_vector\n",
    "        \n",
    "        # emotional mean\n",
    "        for key in emotional_vector:\n",
    "            emotional_vector[key] = np.mean([item[key] for item in emotion_timeline])\n",
    "        emotional_mean = emotional_vector\n",
    "#         print(emotional_mean)\n",
    "        emotional_vector = dup_emotional_vector\n",
    "\n",
    "        # emotional standard deviation\n",
    "        for key in emotional_vector:\n",
    "            emotional_vector[key] = np.std([item[key] for item in emotion_timeline])\n",
    "        emotional_std = emotional_vector\n",
    "#         print(emotional_std)\n",
    "        emotional_vector = dup_emotional_vector  \n",
    "        \n",
    "        # emotional q1\n",
    "        for key in emotional_vector:\n",
    "            emotional_vector[key] = np.quantile([item[key] for item in emotion_timeline], 0.25)\n",
    "        emotional_q1 = emotional_vector\n",
    "#         print(emotional_q1)\n",
    "        emotional_vector = dup_emotional_vector       \n",
    "        \n",
    "        # emotional q2\n",
    "        for key in emotional_vector:\n",
    "            emotional_vector[key] = np.quantile([item[key] for item in emotion_timeline], 0.5)\n",
    "        emotional_q2 = emotional_vector\n",
    "#         print(emotional_q2)\n",
    "        emotional_vector = dup_emotional_vector\n",
    "        \n",
    "        # emotional q3\n",
    "        for key in emotional_vector:\n",
    "            emotional_vector[key] = np.quantile([item[key] for item in emotion_timeline], 0.75)\n",
    "        emotional_q3 = emotional_vector\n",
    "#         print(emotional_q3)\n",
    "        emotional_vector = dup_emotional_vector\n",
    "        \n",
    "        # create user features\n",
    "        if node['timeline']:\n",
    "            followers_count = node['timeline'][0]['user']['followers_count']\n",
    "            friends_count = node['timeline'][0]['user']['friends_count']\n",
    "            listed_count = node['timeline'][0]['user']['listed_count']\n",
    "            statuses_count = node['timeline'][0]['user']['statuses_count']\n",
    "            created_at = node['timeline'][0]['user']['created_at']\n",
    "        \n",
    "        # create a dict\n",
    "        a_dict = {\n",
    "            'user_id' : node_id,\n",
    "            'emotional_frequency' : emotional_frequency,\n",
    "            'emotional_mean' : emotional_mean,\n",
    "            'emotional_std' : emotional_std,\n",
    "            'emotional_q1' : emotional_q1,\n",
    "            'emotional_q2' : emotional_q2,\n",
    "            'emotional_q3' : emotional_q3,\n",
    "            'followers_count': followers_count,\n",
    "            'friends_count': friends_count,\n",
    "            'listed_count': listed_count,\n",
    "            'statuses_count': statuses_count,\n",
    "            'created_at': created_at\n",
    "        }\n",
    "#         print('*'*10)\n",
    "#         print(a_dict)\n",
    "        \n",
    "        return a_dict\n",
    "    \n",
    "    def populate_node_feature_sequence(self, _id):\n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.get_database('tweets')\n",
    "        records = db['cascades']\n",
    "        \n",
    "        _id = ObjectId(_id)\n",
    "        \n",
    "        node_feature_sequence = []\n",
    "        \n",
    "        for node in records.find_one({'_id' : _id})['node_sequence']:\n",
    "            a_dict = self.get_node(node)\n",
    "            node_feature_sequence.append(a_dict)\n",
    "        \n",
    "        print(node_feature_sequence)\n",
    "        # write to database\n",
    "        records.update_one({'_id': _id}, {'$set' : { 'node_feature_sequence' : node_feature_sequence}})\n",
    "    \n",
    "    def populate_me(self):\n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.get_database('tweets')\n",
    "        records = db['cascades']\n",
    "        \n",
    "        for record in records.find({}):\n",
    "            id_str = record['_id']\n",
    "            print(id_str)\n",
    "            self.populate_node_feature_sequence(id_str)\n",
    "        \n",
    "\n",
    "\n",
    "class MultivariateTimeSeriesModal:\n",
    "    series = []\n",
    "    \n",
    "    # reshape data as single dataset where each row is a timestep and each column is a seperate time series\n",
    "    '''\n",
    "    'matrix' : [[]]\n",
    "    '''\n",
    "    def create_vector(self, _id):\n",
    "        \n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.get_database('tweets')\n",
    "        records = db['cascades']\n",
    "        \n",
    "        _id = ObjectId(_id)\n",
    "        \n",
    "        node_feature_sequence = records.find_one({'_id': _id})['node_feature_sequence']\n",
    "#         print(len(node_feature_sequence))\n",
    "        \n",
    "        matrix = []\n",
    "        s = node_feature_sequence\n",
    "        \n",
    "        for time_step in s:\n",
    "            series = self.get_all_values(time_step)\n",
    "            matrix.append(self.series)\n",
    "#         print(matrix)\n",
    "        \n",
    "        # write to database\n",
    "        records.update_one({'_id': _id}, {'$set' : { 'matrix' : matrix}})\n",
    "        \n",
    "    def get_all_values(self, d):   \n",
    "        for key, value in d.items():\n",
    "            if type(value) is dict:\n",
    "                self.get_all_values(value)\n",
    "            else:\n",
    "                self.series.append(value)\n",
    "\n",
    "    def populate_me(self):\n",
    "        # Connect to MongoDB    \n",
    "        client = MongoClient('mongodb://localhost:27017/')\n",
    "        db = client.get_database('tweets')\n",
    "        records = db['cascades']\n",
    "        \n",
    "        for record in records.find({}):\n",
    "            id_str = record['_id']\n",
    "            print(id_str)\n",
    "            self.create_vector(id_str)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3309b137-f245-4b4e-9efe-f1fc378628e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "objt = MultivariateTimeSeriesModal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1dd6fa07-47b3-425d-901f-b1be4a748411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611a751f136ce8c98ad4c242\n",
      "611a751f136ce8c98ad4c243\n",
      "611a751f136ce8c98ad4c244\n",
      "611a751f136ce8c98ad4c245\n",
      "611a751f136ce8c98ad4c246\n",
      "611a751f136ce8c98ad4c247\n",
      "611a751f136ce8c98ad4c248\n",
      "611a751f136ce8c98ad4c249\n",
      "611a751f136ce8c98ad4c24a\n",
      "611a751f136ce8c98ad4c24b\n",
      "611a751f136ce8c98ad4c24c\n",
      "611a751f136ce8c98ad4c24d\n",
      "611a751f136ce8c98ad4c24e\n",
      "611a751f136ce8c98ad4c24f\n",
      "611a751f136ce8c98ad4c250\n",
      "611a751f136ce8c98ad4c251\n",
      "611a751f136ce8c98ad4c252\n",
      "611a751f136ce8c98ad4c253\n",
      "611a751f136ce8c98ad4c254\n",
      "611a751f136ce8c98ad4c255\n",
      "611a751f136ce8c98ad4c256\n",
      "611a751f136ce8c98ad4c257\n",
      "611a751f136ce8c98ad4c258\n",
      "611a751f136ce8c98ad4c259\n",
      "611a751f136ce8c98ad4c25a\n",
      "611a751f136ce8c98ad4c25b\n",
      "611a751f136ce8c98ad4c25c\n",
      "611a751f136ce8c98ad4c25d\n",
      "611a751f136ce8c98ad4c25e\n",
      "611a751f136ce8c98ad4c25f\n",
      "611a751f136ce8c98ad4c260\n",
      "611a751f136ce8c98ad4c261\n",
      "611a751f136ce8c98ad4c262\n",
      "611a751f136ce8c98ad4c263\n",
      "611a751f136ce8c98ad4c264\n",
      "611a751f136ce8c98ad4c265\n",
      "611a751f136ce8c98ad4c266\n",
      "611a751f136ce8c98ad4c267\n",
      "611a751f136ce8c98ad4c268\n",
      "611a751f136ce8c98ad4c269\n",
      "611a751f136ce8c98ad4c26a\n",
      "611a751f136ce8c98ad4c26b\n",
      "611a751f136ce8c98ad4c26c\n",
      "611a751f136ce8c98ad4c26d\n",
      "611a751f136ce8c98ad4c26e\n",
      "611a751f136ce8c98ad4c26f\n",
      "611a751f136ce8c98ad4c270\n",
      "611a751f136ce8c98ad4c271\n",
      "611a751f136ce8c98ad4c272\n",
      "611a751f136ce8c98ad4c273\n",
      "611a751f136ce8c98ad4c274\n",
      "611a751f136ce8c98ad4c275\n",
      "611a751f136ce8c98ad4c276\n",
      "611a751f136ce8c98ad4c277\n",
      "611a751f136ce8c98ad4c278\n",
      "611a751f136ce8c98ad4c279\n",
      "611a751f136ce8c98ad4c27a\n",
      "611a751f136ce8c98ad4c27b\n",
      "611a751f136ce8c98ad4c27c\n",
      "611a751f136ce8c98ad4c27d\n",
      "611a751f136ce8c98ad4c27e\n",
      "611a751f136ce8c98ad4c27f\n",
      "611a751f136ce8c98ad4c280\n",
      "611a751f136ce8c98ad4c281\n",
      "611a751f136ce8c98ad4c282\n",
      "611a751f136ce8c98ad4c283\n",
      "611a751f136ce8c98ad4c284\n",
      "611a751f136ce8c98ad4c285\n",
      "611a751f136ce8c98ad4c286\n",
      "611a751f136ce8c98ad4c287\n",
      "611a751f136ce8c98ad4c288\n",
      "611a751f136ce8c98ad4c289\n",
      "611a751f136ce8c98ad4c28a\n",
      "611a751f136ce8c98ad4c28b\n",
      "611a751f136ce8c98ad4c28c\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'node_feature_sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-271-d277f3e00f91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobjt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate_me\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-269-f2f9e2521483>\u001b[0m in \u001b[0;36mpopulate_me\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[0mid_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-269-f2f9e2521483>\u001b[0m in \u001b[0;36mcreate_vector\u001b[1;34m(self, _id)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0m_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObjectId\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mnode_feature_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_id\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'node_feature_sequence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;31m#         print(len(node_feature_sequence))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'node_feature_sequence'"
     ]
    }
   ],
   "source": [
    "objt.populate_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f3c178e9-4147-4eab-9ad2-bb37b81d1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Cascade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105750c-b46f-4cff-9b9c-5a83ba7ed34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bc1cb941-5a1f-49a5-a033-2ceeab55f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj.populate_node_feature_sequence('611a751f136ce8c98ad4c242')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5a91fd75-c1a6-44b1-9a6c-9d59f69151ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false count:  8847\n",
      "false average number of nodes: 7.127161749745676\n",
      "false max number of nodes 15\n",
      "*****\n",
      "True count:  316\n",
      "true average number of nodes: 3.1835443037974684\n",
      "true max number of nodes 10\n"
     ]
    }
   ],
   "source": [
    "obj.cascade_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcd9333e-055e-46d5-ab7f-f9e80acc9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1,2,3,4,5]\n",
    "# li = np.array(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98b71651-82c0-4b70-8cdf-5fdd278fcb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b5735e69-2956-4c9a-95de-22a5d7e5550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 -1\n",
      "3 2 -1\n",
      "4 3 -1\n",
      "5 4 -1\n"
     ]
    }
   ],
   "source": [
    "for p,c in zip(li[:-1],li[1:]):\n",
    "    print(c, p, p-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "61c6137a-3edd-4f47-9797-c199c9c60198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'values_changed': {\"root['me']\": {'new_value': 3, 'old_value': 5}, \"root['l']['l']\": {'new_value': 3, 'old_value': 10}}, 'deep_distance': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "d1 = {'me': 5, 'l': {'l': 10}}\n",
    "d2 = {'me': 3, 'l': {'l': 3}}\n",
    "\n",
    "print(DeepDiff(d1, d2, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d2188954-0ec1-4f7d-a1dc-5a1e8a755336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = np.array([1,1,1,1]).reshape(2,2)\n",
    "l2 = np.array([2,2,2])\n",
    "\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510d2c2-398b-4241-9a34-07814c471267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
